gensbi.models.flux1.math
========================

.. py:module:: gensbi.models.flux1.math


Functions
---------

.. autoapisummary::

   gensbi.models.flux1.math.apply_rope
   gensbi.models.flux1.math.attention
   gensbi.models.flux1.math.rope


Module Contents
---------------

.. py:function:: apply_rope(xq, xk, freqs_cis)

   
   Apply rotary positional embeddings.

   :param xq: Query tensor.
   :type xq: Array
   :param xk: Key tensor.
   :type xk: Array
   :param freqs_cis: Frequency embeddings.
   :type freqs_cis: Array

   :returns: Transformed query and key tensors.
   :rtype: Tuple[Array, Array]















   ..
       !! processed by numpydoc !!

.. py:function:: attention(q, k, v, pe = None, mask = None)

   
   Compute attention mechanism.

   :param q: Query tensor.
   :type q: Array
   :param k: Key tensor.
   :type k: Array
   :param v: Value tensor.
   :type v: Array
   :param pe: Positional encoding.
   :type pe: Optional[Array]
   :param mask: Attention mask.
   :type mask: Optional[Array]

   :returns: Attention output.
   :rtype: Array















   ..
       !! processed by numpydoc !!

.. py:function:: rope(pos, dim, theta)

   
   Compute rotary positional embeddings.

   :param pos: Position tensor.
   :type pos: Array
   :param dim: Dimension of embeddings.
   :type dim: int
   :param theta: Scaling factor.
   :type theta: int

   :returns: Rotary embeddings.
   :rtype: Array















   ..
       !! processed by numpydoc !!

