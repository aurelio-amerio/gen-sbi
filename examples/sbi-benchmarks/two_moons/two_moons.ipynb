{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load autoreload extension\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['JAX_PLATFORMS']=\"cpu\"\n",
    "os.environ['JAX_PLATFORMS']=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_model=False\n",
    "restore_model=True\n",
    "train_model=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import orbax.checkpoint as ocp\n",
    "checkpoint_dir = \"/lhome/ific/a/aamerio/github/cfm-jax/checkpoints/two_moons_simformer\"\n",
    "\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "if overwrite_model:\n",
    "    checkpoint_dir = ocp.test_utils.erase_and_create_empty(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "import optax\n",
    "from optax.contrib import reduce_on_plateau\n",
    "\n",
    "from numpyro import distributions as dist\n",
    "\n",
    "from corner import corner\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATIENCE = 10  # @param{type:\"integer\"}\n",
    "# @markdown Number of epochs to wait before resuming normal operation after the learning rate reduction:\n",
    "COOLDOWN = 5  # @param{type:\"integer\"}\n",
    "# @markdown Factor by which to reduce the learning rate:\n",
    "FACTOR = 0.5  # @param{type:\"number\"}\n",
    "# @markdown Relative tolerance for measuring the new optimum:\n",
    "RTOL = 1e-4  # @param{type:\"number\"}\n",
    "# @markdown Number of iterations to accumulate an average value:\n",
    "ACCUMULATION_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "root = \"/lhome/ific/a/aamerio/github/cfm-jax\"\n",
    "sys.path.append(f\"{root}/examples/sbi-benchmarks\")\n",
    "sys.path.append(f\"{root}/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sbi_tasks import TwoMoons\n",
    "from utils.dataloader import InfiniteDataLoader\n",
    "\n",
    "from flow_matching.path.scheduler import CondOTScheduler\n",
    "from flow_matching.path import AffineProbPath\n",
    "from flow_matching.solver import ODESolver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twomoons = TwoMoons()\n",
    "task = twomoons.task\n",
    "prior = twomoons.get_prior()\n",
    "simulator = twomoons.get_simulator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=[\"size\"])\n",
    "def sample_prior(size):\n",
    "    return jnp.array(prior.sample((size,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_samples = task.get_reference_posterior_samples(num_observation=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(reference_samples[:, 0], reference_samples[:, 1], bins=(200,200), range=[(-1, 1), (-1, 1)], density=True)\n",
    "plt.xlim((-1,1))\n",
    "plt.ylim((-1,1))\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataset\n",
    "nsamples = int(1e6)\n",
    "val_size = 512\n",
    "theta = prior.sample((nsamples+val_size,))\n",
    "xs = jnp.array(simulator(theta))\n",
    "node_ids = jnp.array(twomoons.get_node_id())\n",
    "dim_theta = twomoons.get_theta_dim()\n",
    "dim_x = twomoons.get_x_dim()\n",
    "\n",
    "# turn them into jax arrays\n",
    "theta = jnp.array(theta)\n",
    "xs = jnp.array(xs)\n",
    "\n",
    "# concatenate the data, theta and xs\n",
    "data = jnp.concatenate((theta, xs), axis=-1)\n",
    "\n",
    "\n",
    "train_data = data[:nsamples]\n",
    "val_data = data[nsamples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "dataset = InfiniteDataLoader(train_data, batch_size, rng=nnx.Rngs(0).dataset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define the CFM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Simformer, SimformerParams, SimformerCFMLoss, SimformerConditioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = AffineProbPath(scheduler=CondOTScheduler()) # define the probability path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_joint = len(twomoons.get_node_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = SimformerParams(\n",
    "    rngs = nnx.Rngs(0),\n",
    "    dim_value = 40,\n",
    "    dim_id = 40, \n",
    "    dim_condition = 10, \n",
    "    dim_joint= dim_joint,\n",
    "    fourier_features = 128,\n",
    "    num_heads = 4,\n",
    "    num_layers = 6,\n",
    "    widening_factor = 3,\n",
    "    qkv_features = 8, # this bottlenecks the transformer features to 8, instead of the token dimension\n",
    "    num_hidden_layers = 1,\n",
    "    dropout_rate = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn_cfm = SimformerCFMLoss(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginalize(rng: jax.random.PRNGKey, edge_mask: jax.Array):\n",
    "    # Simple function that marginializes out a single node from a adjacency matrix of a graph.\n",
    "    idx = jax.random.choice(rng, jnp.arange(edge_mask.shape[0]), shape=(1,), replace=False)\n",
    "    edge_mask = edge_mask.at[idx, :].set(False)\n",
    "    edge_mask = edge_mask.at[:, idx].set(False)\n",
    "    edge_mask = edge_mask.at[idx, idx].set(True)\n",
    "    return edge_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undirected_edge_mask = twomoons.get_edge_mask_fn(\"undirected\")(node_ids, None)\n",
    "posterior_faithfull = twomoons.get_edge_mask_fn(\"faithfull\")(node_ids, condition_mask=jnp.array([0,0,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=[\"nsamples\"])\n",
    "def get_random_condition_mask(rng: jax.random.PRNGKey, nsamples):\n",
    "    mask_joint = jnp.zeros((5*nsamples, dim_joint ), dtype=jnp.bool_)\n",
    "    mask_posterior = jnp.concatenate([jnp.zeros((nsamples, dim_theta), dtype=jnp.bool_), jnp.ones((nsamples, dim_x), dtype=jnp.bool_)], axis=-1)\n",
    "    # mask_likelihood = jnp.concatenate([jnp.ones((nsamples, dim_theta), dtype=jnp.bool_), jnp.zeros((nsamples, dim_x), dtype=jnp.bool_)], axis=-1)\n",
    "    \n",
    "    mask1 = jax.random.bernoulli(rng, p=0.3, shape=(nsamples, dim_joint))\n",
    "    filter = ~jnp.all(mask1, axis=-1)\n",
    "    mask1 = jnp.logical_and(mask1, filter.reshape(-1,1))\n",
    "\n",
    "    # masks = jnp.concatenate([mask_joint, mask1, mask_posterior, mask_likelihood], axis=0)\n",
    "    masks = jnp.concatenate([mask_joint, mask1, mask_posterior], axis=0)\n",
    "    return  jax.random.choice(rng, masks, shape=(nsamples,), replace=False, axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0_dist_model = dist.Independent(\n",
    "    dist.Normal(loc=jnp.zeros((4,)), scale=jnp.ones((4,))),\n",
    "    reinterpreted_batch_ndims=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_(vf_model, x_1, key: jax.random.PRNGKey):\n",
    "\n",
    "    batch_size = x_1.shape[0]\n",
    "\n",
    "    rng_x0, rng_t, rng_condition, rng_edge_mask1, rng_edge_mask2 = jax.random.split(key, 5)\n",
    "    \n",
    "    # Generate data and random times\n",
    "    x_0 = p0_dist_model.sample(rng_x0, (batch_size,)) # n, T_max, 1\n",
    "    \n",
    "    t = jax.random.uniform(rng_t, x_1.shape[0])\n",
    "\n",
    "    batch = (x_0, x_1, t)\n",
    "    \n",
    "    # Condition mask -> randomly condition on some data. Here you can choose between the different condition masks, and you should specify the conditionals you may want to compute afterwards.\n",
    "    condition_mask = get_random_condition_mask(rng_condition, batch_size)\n",
    "\n",
    "    # undirected_edge_mask \n",
    "    undirected_edge_mask_ = jnp.repeat(undirected_edge_mask[None,...], 3*batch_size, axis=0) # Dense default mask\n",
    "    \n",
    "    # faithfull posterior mask\n",
    "    faithfull_edge_mask_ = jnp.repeat(posterior_faithfull[None,...], 3*batch_size, axis=0) # Dense default mask\n",
    "    \n",
    "    # Include marginal consistency\n",
    "    marginal_mask = jax.vmap(marginalize, in_axes=(0,None))(jax.random.split(rng_edge_mask1, (batch_size,)), undirected_edge_mask)\n",
    "    edge_masks = jnp.concatenate([undirected_edge_mask_, faithfull_edge_mask_, marginal_mask], axis=0)\n",
    "    edge_masks = jax.random.choice(rng_edge_mask2, edge_masks, shape=(batch_size,), axis=0) # Randomly choose between dense and marginal mask\n",
    "\n",
    "\n",
    "    # Forward diffusion, do not perturb conditioned data\n",
    "    # Will use the condition mask to mask to prevent adding noise for nodes that are conditioned.\n",
    "    loss = loss_fn_cfm(vf_model, batch, node_ids=node_ids, edge_mask=edge_masks,condition_mask=condition_mask, )\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def train_loss(vf_model, key: jax.random.PRNGKey):\n",
    "    x_1 = next(dataset) # n, T_max, 1\n",
    "    return loss_fn_(vf_model, x_1, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def val_loss(vf_model, key):\n",
    "    x_1 = val_data\n",
    "    return loss_fn_(vf_model, x_1, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def train_step(model, optimizer, rng):\n",
    "    loss_fn = lambda model: train_loss(model, rng)\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "    optimizer.update(grads, value=loss)  # In place updates.\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf_model = Simformer(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â restore the model\n",
    "if restore_model:\n",
    "    model_state = nnx.state(vf_model)\n",
    "    graphdef, abstract_state = nnx.split(vf_model)\n",
    "\n",
    "    with ocp.CheckpointManager(\n",
    "        checkpoint_dir, options=ocp.CheckpointManagerOptions(read_only=True)\n",
    "    ) as read_mgr:\n",
    "        restored = read_mgr.restore(\n",
    "            1,\n",
    "            # pass in the model_state to restore the exact same State type\n",
    "            args=ocp.args.Composite(state=ocp.args.PyTreeRestore(item=model_state))\n",
    "        )\n",
    "\n",
    "    vf_model= nnx.merge(graphdef, restored[\"state\"])\n",
    "    print(\"Restored model from checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear learning rate schedule\n",
    "# nsteps = 10_000 \n",
    "# nepochs = 5\n",
    "\n",
    "\n",
    "# schedule = optax.schedules.linear_schedule(1e-3, 1e-6, 45000, 5000)\n",
    "\n",
    "# opt = optax.chain(optax.adaptive_grad_clip(10.0), optax.adamw(schedule))\n",
    "\n",
    "# # opt = optax.MultiSteps(opt, 2)\n",
    "\n",
    "# optimizer = nnx.Optimizer(vf_model, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce on plateau schedule\n",
    "nsteps = 10_000 \n",
    "nepochs = 5\n",
    "\n",
    "multistep=2\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.adaptive_grad_clip(10.0),\n",
    "    optax.adamw(1e-3),\n",
    "    reduce_on_plateau( \n",
    "        patience=PATIENCE,\n",
    "        cooldown=COOLDOWN,\n",
    "        factor=FACTOR,\n",
    "        rtol=RTOL,\n",
    "        accumulation_size=ACCUMULATION_SIZE,\n",
    "    ),\n",
    ")\n",
    "\n",
    "opt = optax.MultiSteps(opt, multistep)\n",
    "optimizer = nnx.Optimizer(vf_model, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rngs = nnx.Rngs(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_state = nnx.state(vf_model)\n",
    "best_val_loss_value = val_loss(vf_model, jax.random.PRNGKey(0))\n",
    "val_error_ratio = 1.1\n",
    "counter = 0\n",
    "cmax = 10\n",
    "print_every = 100\n",
    "\n",
    "loss_array = []\n",
    "val_loss_array = []\n",
    "\n",
    "early_stopping = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model:\n",
    "    vf_model.train()\n",
    "\n",
    "    for ep in range(nepochs):\n",
    "        pbar = tqdm(range(nsteps))\n",
    "        l = 0\n",
    "        v_l = 0\n",
    "        for j in pbar:\n",
    "            if counter > cmax and early_stopping:\n",
    "                print(\"Early stopping\")\n",
    "                # restore the model state\n",
    "                graphdef, abstract_state = nnx.split(vf_model)\n",
    "\n",
    "                vf_model = nnx.merge(graphdef, best_state)\n",
    "                break\n",
    "            \n",
    "            loss = train_step(vf_model, optimizer, rngs.train_step())\n",
    "            l += loss.item()\n",
    "            if j>0 and j % 10 == 0:\n",
    "                v_loss = val_loss(vf_model, rngs.val_step())\n",
    "                v_l += v_loss.item()\n",
    "                \n",
    "            if j>0 and j % 100 == 0:\n",
    "                loss_ = l/100\n",
    "                val_ = v_l/10\n",
    "\n",
    "                ratio1 = val_ / loss_\n",
    "                ratio2 = val_ / best_val_loss_value\n",
    "                \n",
    "                # if ratio1 < val_error_ratio and ratio2 < 1.05:\n",
    "                if ratio1 < val_error_ratio:\n",
    "                    if val_ < best_val_loss_value:\n",
    "                        best_val_loss_value = val_\n",
    "                        best_state = nnx.state(vf_model)\n",
    "                    counter = 0\n",
    "                else:\n",
    "                    counter += 1\n",
    "                \n",
    "                # scale = tree_get(optimizer.opt_state, \"ReduceLROnPlateauState\").scale.value\n",
    "                # pbar.set_postfix(loss=f\"{l/(100):.4f}\", ratio=f\"{ratio:.4f}\", counter=counter, lr_scale=scale, val_loss = f\"{val_:.4f}\" )\n",
    "                pbar.set_postfix(loss=f\"{loss_:.4f}\", ratio=f\"{ratio1:.4f}\", counter=counter, val_loss = f\"{val_:.4f}\" )\n",
    "                loss_array.append(loss_)\n",
    "                val_loss_array.append(val_) \n",
    "                l=0\n",
    "                v_l=0\n",
    "\n",
    "            # if j>0 and j%3000 == 0:\n",
    "            #     idx = 8\n",
    "            #     vf_wrapped = FluxWrapper(vf_model)\n",
    "            #     samples, true_param, reference_samples = get_samples(vf_wrapped, idx)\n",
    "            #     plot_samples(samples, true_param)\n",
    "        # print(l)\n",
    "\n",
    "\n",
    "\n",
    "    vf_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "if train_model:\n",
    "    checkpoint_manager = ocp.CheckpointManager(checkpoint_dir,\n",
    "        options=ocp.CheckpointManagerOptions(\n",
    "            max_to_keep=2,\n",
    "            keep_checkpoints_without_metrics=True,\n",
    "            create=True,\n",
    "        ),\n",
    "    )\n",
    "    model_state = nnx.state(vf_model)\n",
    "    checkpoint_manager.save(\n",
    "        1, args=ocp.args.Composite(state=ocp.args.PyTreeSave(model_state))\n",
    "    )\n",
    "\n",
    "    checkpoint_manager.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_array, label=\"train loss\")\n",
    "plt.plot(val_loss_array, label=\"val loss\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow_matching.utils import ModelWrapper, GuidedModelWrapper\n",
    "class SimWrapper(ModelWrapper):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(model)\n",
    "\n",
    "    def __call__(self, x, t, args, **kwargs):\n",
    "        return self.model(obs=x, timesteps=t, **kwargs)\n",
    "\n",
    "class GuidedSimWrapper(GuidedModelWrapper):\n",
    "    def __init__(self, model, cfg_scale):\n",
    "        super().__init__(model, cfg_scale)\n",
    "\n",
    "    def __call__(self, x, t, args, **kwargs):\n",
    "        return self.model(obs=x, timesteps=t, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_ids = jnp.array([0, 1])\n",
    "cond_ids = jnp.array([2, 3])\n",
    "step_size = 0.01\n",
    "\n",
    "# conditional sampling\n",
    "def get_samples(vf_wrapped, idx, nsamples=10_000, edge_mask=posterior_faithfull):\n",
    "    observation =  jnp.array(twomoons.get_observation(idx))\n",
    "    true_param = jnp.array(task.get_true_parameters(idx))\n",
    "    reference_samples = task.get_reference_posterior_samples(num_observation=idx)\n",
    "\n",
    "    rng = jax.random.PRNGKey(45)\n",
    "\n",
    "    key1,key2 = jax.random.split(rng, 2)\n",
    "\n",
    "    x_init = jax.random.normal(key1,(nsamples, dim_theta)) # n, T_max, 1\n",
    "    cond = jnp.broadcast_to(observation[...,None], (1, dim_x, 1)) # n, dim_theta, 1\n",
    "\n",
    "    solver = ODESolver(velocity_model=vf_wrapped)  # create an ODESolver class\n",
    "    model_extras = {\"cond\": cond, \"obs_ids\": obs_ids, \"cond_ids\": cond_ids, \"edge_mask\": edge_mask}\n",
    "\n",
    "    sampler_ = solver.get_sampler(method='Dopri5', step_size=step_size, return_intermediates=False, model_extras=model_extras)\n",
    "    samples = sampler_(x_init)  # sample from the model\n",
    "\n",
    "    return samples, true_param, reference_samples\n",
    "\n",
    "def plot_samples(samples, true_param):\n",
    "    plt.hist2d(samples[:,0], samples[:,1], bins=(200,200), range=[(-1, 1), (-1, 1)], density=True)\n",
    "    # same ratio on axis \n",
    "    plt.scatter(true_param[0,0], true_param[0,1], s=100, color='red', alpha=0.5, marker='x')\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.xlim((-1,1))\n",
    "    plt.ylim((-1,1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf_cond = SimformerConditioner(vf_model)\n",
    "vf_wrapped = SimWrapper(vf_cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to do conditional inference. We need an observation for which we want to ocmpute the posterior\n",
    "idx=8\n",
    "observation = jnp.array(twomoons.get_observation(idx))\n",
    "reference_samples = task.get_reference_posterior_samples(num_observation=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = get_samples(vf_wrapped, idx, nsamples=100_000)[0]\n",
    "# samples, true_param, reference_samples = get_samples(vf_wrapped, idx, nsamples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples = samples\n",
    "\n",
    "plt.hist2d(posterior_samples[:,0], posterior_samples[:,1], bins=100, range=[(-1, 1), (-1, 1)], density=True)\n",
    "# same ratio on axis \n",
    "# plt.scatter(reference_samples[:,0], reference_samples[:,1], s=0.1, color='red', alpha=0.1, marker='x')\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "# plt.xlim((-1,1))\n",
    "# plt.ylim((-1,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner(np.array(posterior_samples), bins=100, smooth=True, range=[(-1, 1), (-1, 1)], labels=['$\\\\theta_1$', '$\\\\theta_2$'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner(np.array(reference_samples), bins=100, smooth=True, range=[(-1, 1), (-1, 1)], labels=['$\\\\theta_1$', '$\\\\theta_2$'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 8\n",
    "observation = jnp.array(twomoons.get_observation(idx))\n",
    "solver = ODESolver(velocity_model=vf_wrapped)  # create an ODESolver class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0_cond = dist.Independent(\n",
    "    dist.Normal(loc=jnp.zeros((2,)), scale=jnp.ones((2,))),\n",
    "    reinterpreted_batch_ndims=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 200\n",
    "x_1 = jnp.meshgrid(jnp.linspace(-1, 1, grid_size), jnp.linspace(-1, 1, grid_size))\n",
    "x_1 = jnp.stack([x_1[0].flatten(), x_1[1].flatten()], axis=1)\n",
    "\n",
    "# cond = jnp.broadcast_to(observation[...,None], (x_1.shape[0], dim_theta,1)) # n, dim_theta, 1\n",
    "# cond = jnp.broadcast_to(observation, (x_1.shape[0], dim_theta)) # n, dim_theta, 1\n",
    "cond = jnp.broadcast_to(observation, (1, dim_theta)) # n, dim_theta, 1\n",
    "\n",
    "obs_ids = jnp.array([0, 1])\n",
    "cond_ids = jnp.array([2, 3])\n",
    "model_extras = {\"cond\": cond, \"obs_ids\": obs_ids, \"cond_ids\": cond_ids, \"edge_mask\": posterior_faithfull}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the logprob\n",
    "# logp_sampler = solver.get_unnormalized_logprob(condition_mask=condition_mask, time_grid=[1.0,0.0],method='Dopri5', step_size=step_size, log_p0=p0_dist_model.log_prob, model_extras=model_extras)\n",
    "logp_sampler = solver.get_unnormalized_logprob(time_grid=[1.0,0.0],method='Dopri5', step_size=step_size, log_p0=p0_cond.log_prob, model_extras=model_extras)\n",
    "# create an y_init which has theta on the first position and x1,x2 on the second and third position\n",
    "\n",
    "# y_init = p0_cond.sample(jax.random.PRNGKey(0), (x_1.shape[0],))  # n, dim_theta\n",
    "y_init = x_1\n",
    "\n",
    "exact_log_p = logp_sampler(y_init)\n",
    "p = jnp.exp(exact_log_p)[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_grid = p.reshape((grid_size, grid_size))\n",
    "plt.imshow(p_grid, origin='lower', aspect='auto', extent=(-1, 1, -1, 1), cmap='viridis')\n",
    "plt.xlim((-1,1))\n",
    "plt.ylim((-1,1))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample with guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gvf_wrapped = GuidedSimWrapper(vf_cond, cfg_scale=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to do conditional inference. We need an observation for which we want to ocmpute the posterior\n",
    "idx=8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = get_samples(gvf_wrapped, idx, nsamples=100_000, edge_mask=None)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples = samples\n",
    "\n",
    "plt.hist2d(posterior_samples[:,0], posterior_samples[:,1], bins=100, range=[(-1, 1), (-1, 1)], density=True)\n",
    "# same ratio on axis \n",
    "# plt.scatter(reference_samples[:,0], reference_samples[:,1], s=0.1, color='red', alpha=0.1, marker='x')\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "# plt.xlim((-1,1))\n",
    "# plt.ylim((-1,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner(np.array(posterior_samples), bins=100, smooth=True, range=[(-1, 1), (-1, 1)], labels=['$\\\\theta_1$', '$\\\\theta_2$'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C2ST test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbibm.metrics import c2st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert posterior samples to a torch array\n",
    "import torch\n",
    "idx = 2\n",
    "reference_samples = task.get_reference_posterior_samples(num_observation=idx)\n",
    "posterior_samples = get_samples(vf_wrapped, idx, nsamples=reference_samples.shape[0])[0]\n",
    "posterior_samples_torch = torch.tensor(np.array(posterior_samples), dtype=torch.float32)\n",
    "\n",
    "posterior_samples_cfg = get_samples(gvf_wrapped, idx, nsamples=reference_samples.shape[0], edge_mask=None)[0]\n",
    "posterior_samples_cfg_torch = torch.tensor(np.array(posterior_samples_cfg), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2st_accuracy = c2st(reference_samples, posterior_samples_torch)\n",
    "c2st_accuracy_cfg = c2st(reference_samples, posterior_samples_cfg_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2st_accuracy, c2st_accuracy_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
